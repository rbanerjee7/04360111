{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a33a1f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "# Torch package\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "import pennylane as qml\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "31c36071",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(15)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "plt.rcParams.update({'font.size':20})\n",
    "legend_prop = {'weight':'bold'}\n",
    "from pylab import rcParams\n",
    "rcParams['axes.linewidth'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111505b5-5d5f-4e55-8e47-ecfdda0593cc",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d53abe07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Processed_data_till_1_march.csv')\n",
    "df = df.drop([ 'Unnamed: 0'], axis=1)\n",
    "# Utilize data from Sep 2011 till 1st March 2024\n",
    "df=df[50:]\n",
    "# Remove few redundant data\n",
    "df=df.drop(columns=['Date'])\n",
    "df=df.drop(columns=['CUMLOGRET_1'])\n",
    "df=df.drop(columns=['Gold in USD volume'])\n",
    "df=df.drop(columns=['Open'])\n",
    "df=df.drop(columns=['High'])\n",
    "df=df.drop(columns=['Low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8cedb86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3M', 'BBB_20_2.0', 'BBL_20_2.0', 'BBM_20_2.0', 'BBP_20_2.0', 'BBU_20_2.0', 'Close_copy', 'Crude Futures_close', 'Crude Futures_volume', 'Crude_H-L', 'Crude_O-C', 'EMA_14', 'EMA_21', 'EMA_7', 'FTSE_H-L', 'FTSE_O-C', 'GBP USD ', 'GBP_USD_H-L', 'GBP_USD_O-C', 'Gold in USD close', 'Gold_H-L', 'Gold_O-C', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'SMA_14', 'SMA_21', 'SMA_7', 'Volume']\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# Set target and features\n",
    "target = \"Close\"\n",
    "features = list(df.columns.difference([\"Close\"]))\n",
    "print(features)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11e95ff1-300d-4e80-8f1a-fc0c2ba35d0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_mean = df[target].mean()\n",
    "target_stdev = df[target].std()\n",
    "\n",
    "mean=dict()\n",
    "stdev=dict()\n",
    "for c in df.columns:\n",
    "    mean[c] = df[c].mean()\n",
    "    stdev[c] = df[c].std()\n",
    "\n",
    "    df[c] = (df[c] - mean[c]) / stdev[c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6e581d-4e02-485a-a2cf-3a754e51b966",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>EMA_7</th>\n",
       "      <th>EMA_14</th>\n",
       "      <th>EMA_21</th>\n",
       "      <th>SMA_7</th>\n",
       "      <th>SMA_14</th>\n",
       "      <th>SMA_21</th>\n",
       "      <th>MACD_12_26_9</th>\n",
       "      <th>MACDh_12_26_9</th>\n",
       "      <th>...</th>\n",
       "      <th>3M</th>\n",
       "      <th>FTSE_H-L</th>\n",
       "      <th>FTSE_O-C</th>\n",
       "      <th>GBP_USD_H-L</th>\n",
       "      <th>GBP_USD_O-C</th>\n",
       "      <th>Gold_H-L</th>\n",
       "      <th>Gold_O-C</th>\n",
       "      <th>Crude_H-L</th>\n",
       "      <th>Crude_O-C</th>\n",
       "      <th>Close_copy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>-2.216035</td>\n",
       "      <td>0.076483</td>\n",
       "      <td>-2.142262</td>\n",
       "      <td>-2.152261</td>\n",
       "      <td>-2.194267</td>\n",
       "      <td>-2.125236</td>\n",
       "      <td>-2.076858</td>\n",
       "      <td>-2.136731</td>\n",
       "      <td>0.580627</td>\n",
       "      <td>-0.832570</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051602</td>\n",
       "      <td>1.239983</td>\n",
       "      <td>0.263215</td>\n",
       "      <td>0.069296</td>\n",
       "      <td>-0.504898</td>\n",
       "      <td>-0.245371</td>\n",
       "      <td>0.621244</td>\n",
       "      <td>0.493902</td>\n",
       "      <td>0.076677</td>\n",
       "      <td>-2.216035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>-2.056965</td>\n",
       "      <td>-0.239245</td>\n",
       "      <td>-2.123602</td>\n",
       "      <td>-2.142163</td>\n",
       "      <td>-2.184183</td>\n",
       "      <td>-2.111332</td>\n",
       "      <td>-2.077166</td>\n",
       "      <td>-2.125910</td>\n",
       "      <td>0.587184</td>\n",
       "      <td>-0.650028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049936</td>\n",
       "      <td>0.645604</td>\n",
       "      <td>-1.592823</td>\n",
       "      <td>0.546749</td>\n",
       "      <td>0.078090</td>\n",
       "      <td>-0.237026</td>\n",
       "      <td>-0.837124</td>\n",
       "      <td>-0.175958</td>\n",
       "      <td>0.061469</td>\n",
       "      <td>-2.056965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>-2.098709</td>\n",
       "      <td>-0.344334</td>\n",
       "      <td>-2.120115</td>\n",
       "      <td>-2.139050</td>\n",
       "      <td>-2.178879</td>\n",
       "      <td>-2.117365</td>\n",
       "      <td>-2.077906</td>\n",
       "      <td>-2.121902</td>\n",
       "      <td>0.551176</td>\n",
       "      <td>-0.605426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049715</td>\n",
       "      <td>0.155241</td>\n",
       "      <td>0.435721</td>\n",
       "      <td>0.581028</td>\n",
       "      <td>6.329870</td>\n",
       "      <td>-1.246710</td>\n",
       "      <td>0.090208</td>\n",
       "      <td>-0.326489</td>\n",
       "      <td>0.989154</td>\n",
       "      <td>-2.098709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>-2.101239</td>\n",
       "      <td>0.232217</td>\n",
       "      <td>-2.118136</td>\n",
       "      <td>-2.136693</td>\n",
       "      <td>-2.174291</td>\n",
       "      <td>-2.119588</td>\n",
       "      <td>-2.081983</td>\n",
       "      <td>-2.115752</td>\n",
       "      <td>0.513769</td>\n",
       "      <td>-0.573078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049288</td>\n",
       "      <td>0.938548</td>\n",
       "      <td>0.039596</td>\n",
       "      <td>0.103050</td>\n",
       "      <td>0.078090</td>\n",
       "      <td>0.572390</td>\n",
       "      <td>-0.203051</td>\n",
       "      <td>-1.432887</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>-2.101239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>-2.114521</td>\n",
       "      <td>0.606201</td>\n",
       "      <td>-2.119995</td>\n",
       "      <td>-2.136444</td>\n",
       "      <td>-2.171350</td>\n",
       "      <td>-2.119997</td>\n",
       "      <td>-2.105306</td>\n",
       "      <td>-2.108239</td>\n",
       "      <td>0.467045</td>\n",
       "      <td>-0.569404</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048312</td>\n",
       "      <td>0.724147</td>\n",
       "      <td>0.148211</td>\n",
       "      <td>-0.139122</td>\n",
       "      <td>1.708155</td>\n",
       "      <td>0.405500</td>\n",
       "      <td>0.288356</td>\n",
       "      <td>0.019731</td>\n",
       "      <td>0.152717</td>\n",
       "      <td>-2.114521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Close    Volume     EMA_7    EMA_14    EMA_21     SMA_7    SMA_14  \\\n",
       "50 -2.216035  0.076483 -2.142262 -2.152261 -2.194267 -2.125236 -2.076858   \n",
       "51 -2.056965 -0.239245 -2.123602 -2.142163 -2.184183 -2.111332 -2.077166   \n",
       "52 -2.098709 -0.344334 -2.120115 -2.139050 -2.178879 -2.117365 -2.077906   \n",
       "53 -2.101239  0.232217 -2.118136 -2.136693 -2.174291 -2.119588 -2.081983   \n",
       "54 -2.114521  0.606201 -2.119995 -2.136444 -2.171350 -2.119997 -2.105306   \n",
       "\n",
       "      SMA_21  MACD_12_26_9  MACDh_12_26_9  ...        3M  FTSE_H-L  FTSE_O-C  \\\n",
       "50 -2.136731      0.580627      -0.832570  ... -0.051602  1.239983  0.263215   \n",
       "51 -2.125910      0.587184      -0.650028  ... -0.049936  0.645604 -1.592823   \n",
       "52 -2.121902      0.551176      -0.605426  ... -0.049715  0.155241  0.435721   \n",
       "53 -2.115752      0.513769      -0.573078  ... -0.049288  0.938548  0.039596   \n",
       "54 -2.108239      0.467045      -0.569404  ... -0.048312  0.724147  0.148211   \n",
       "\n",
       "    GBP_USD_H-L  GBP_USD_O-C  Gold_H-L  Gold_O-C  Crude_H-L  Crude_O-C  \\\n",
       "50     0.069296    -0.504898 -0.245371  0.621244   0.493902   0.076677   \n",
       "51     0.546749     0.078090 -0.237026 -0.837124  -0.175958   0.061469   \n",
       "52     0.581028     6.329870 -1.246710  0.090208  -0.326489   0.989154   \n",
       "53     0.103050     0.078090  0.572390 -0.203051  -1.432887   0.008242   \n",
       "54    -0.139122     1.708155  0.405500  0.288356   0.019731   0.152717   \n",
       "\n",
       "    Close_copy  \n",
       "50   -2.216035  \n",
       "51   -2.056965  \n",
       "52   -2.098709  \n",
       "53   -2.101239  \n",
       "54   -2.114521  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "465d24a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict for next 6 days using last 60 days data\n",
    "sequence_length = 60\n",
    "window =6\n",
    "\n",
    "def create_dataset(dataset,target,features, lookback, window):\n",
    "    X_store, y_store = [], []\n",
    "    for i in range(0,len(dataset)-lookback-window,3):\n",
    "        X = (dataset[features].values)[i:i+lookback]\n",
    "        y = (dataset[target].values)[i+lookback:i+lookback+window]\n",
    "        X_store.append(X)\n",
    "        y_store.append(y)\n",
    "    return torch.FloatTensor(np.array(X_store)), torch.FloatTensor(np.array(y_store))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3dd779-b16d-42ba-8a5d-de7320dfb63d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sample 3208\n",
      "Train sample 3198\n",
      "Test sample 10\n"
     ]
    }
   ],
   "source": [
    "# Split train test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# df_train, df_test = train_test_split(df, test_size=0.05)\n",
    "size=int(len(df))-10\n",
    "df_train=df[:size]\n",
    "df_test=df[size:]\n",
    "print('Total sample', len(df))\n",
    "print('Train sample', len(df_train))\n",
    "print('Test sample', len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d9dd35-23f7-4dc4-91ba-098187905bff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1044, 60, 29]) torch.Size([1044, 6])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-07 19:48:04.650715: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "X_train, y_train = create_dataset(\n",
    "    df_train,\n",
    "    target=target,\n",
    "    features=features,\n",
    "    lookback=sequence_length, window=window)\n",
    "\n",
    "indices = tf.range(start=0, limit=tf.shape(X_train)[0])\n",
    "shuffled_indices = tf.random.shuffle(indices)\n",
    "X_train = tf.gather(X_train, shuffled_indices)\n",
    "y_train = tf.gather(y_train, shuffled_indices)\n",
    "\n",
    "X_train=torch.FloatTensor(np.array(X_train))\n",
    "y_train=torch.FloatTensor(np.array(y_train))\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8858fa34-b301-43da-b1be-51489ed010a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=10\n",
    "train_loader = DataLoader(data.TensorDataset(X_train, y_train), shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d39842-b139-40fb-a164-d8b8f452010f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b15a96e7",
   "metadata": {},
   "source": [
    "## QLSTM-DRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17e97ab8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(data_loader, model, loss_function, optimizer):\n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "    \n",
    "    for X, y in data_loader:\n",
    "        output = model(X)\n",
    "        loss = loss_function(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Train loss: {avg_loss}\")\n",
    "    return avg_loss\n",
    "\n",
    "def test_model(data_loader, model, loss_function):\n",
    "    \n",
    "    num_batches = len(data_loader)\n",
    "    total_loss = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            output = model(X)\n",
    "            total_loss += loss_function(output, y).item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Test loss: {avg_loss}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32c8e360",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_size, \n",
    "                hidden_size, \n",
    "                n_qubits=6, # number of Qubits\n",
    "                n_inp_size=18, # Decide the number of input to quantum model\n",
    "                n_qlayers=12, #  \n",
    "                n_vrotations=3,\n",
    "                batch_first=True,\n",
    "                return_sequences=False, \n",
    "                return_state=False,\n",
    "                backend=\"default.qubit\"):\n",
    "        super(QLSTM, self).__init__()\n",
    "        self.n_inputs = input_size \n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_inp_size=n_inp_size\n",
    "        self.concat_size = self.n_inputs + self.hidden_size\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_qlayers = n_qlayers\n",
    "        self.n_vrotations = n_vrotations\n",
    "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
    "\n",
    "        self.batch_first = batch_first\n",
    "        self.return_sequences = return_sequences\n",
    "        self.return_state = return_state\n",
    "        \n",
    "        self.wires_forget = [f\"wire_forget_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_input = [f\"wire_input_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_update = [f\"wire_update_{i}\" for i in range(self.n_qubits)]\n",
    "        self.wires_output = [f\"wire_output_{i}\" for i in range(self.n_qubits)]\n",
    "\n",
    "        self.dev_forget = qml.device(self.backend, wires=self.wires_forget, shots=None)\n",
    "        self.dev_input = qml.device(self.backend, wires=self.wires_input, shots=None)\n",
    "        self.dev_update = qml.device(self.backend, wires=self.wires_update, shots=None)\n",
    "        self.dev_output = qml.device(self.backend, wires=self.wires_output, shots=None)\n",
    "         \n",
    "        # 6 qubit DRC circuit    \n",
    "        def VQC(inputs, weights, wires_type): # inputs, weights, self.wires_update   \n",
    "            for p1,p2,p3,p4,p5,p6 in zip(weights[:2],weights[2:4],weights[4:6],weights[6:8],weights[8:10],weights[10:12]):\n",
    "                qml.Rot(*inputs[:3], wires=wires_type[0])\n",
    "                qml.Rot(*inputs[3:6], wires=wires_type[1])\n",
    "                qml.Rot(*inputs[6:9], wires=wires_type[2])\n",
    "                qml.Rot(*inputs[9:12], wires=wires_type[3])\n",
    "                qml.Rot(*inputs[12:15], wires=wires_type[4])\n",
    "                qml.Rot(*inputs[15:18], wires=wires_type[5])\n",
    "                qml.Rot(*p1, wires=wires_type[0])\n",
    "                qml.Rot(*p2, wires=wires_type[1])\n",
    "                qml.Rot(*p3, wires=wires_type[2])\n",
    "                qml.Rot(*p4, wires=wires_type[3])\n",
    "                qml.Rot(*p5, wires=wires_type[4])\n",
    "                qml.Rot(*p6, wires=wires_type[5])\n",
    "                qml.CNOT(wires=[wires_type[0], wires_type[1]])\n",
    "                qml.CNOT(wires=[wires_type[1], wires_type[2]])\n",
    "                qml.CNOT(wires=[wires_type[2], wires_type[3]]) \n",
    "                qml.CNOT(wires=[wires_type[3], wires_type[4]])\n",
    "                qml.CNOT(wires=[wires_type[4], wires_type[5]])\n",
    "               \n",
    "        def _circuit_forget(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_forget)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_forget]\n",
    "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\", diff_method='backprop')\n",
    "\n",
    "        def _circuit_input(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_input)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_input]\n",
    "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\", diff_method='backprop')\n",
    "\n",
    "        def _circuit_update(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_update)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_update]\n",
    "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\", diff_method='backprop')\n",
    "\n",
    "        def _circuit_output(inputs, weights):\n",
    "            VQC(inputs, weights, self.wires_output)\n",
    "            return [qml.expval(qml.PauliZ(wires=i)) for i in self.wires_output]\n",
    "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\", diff_method='backprop')\n",
    "\n",
    "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_vrotations)}\n",
    "        print(f\"weight_shapes = (n_qlayers, n_vrotations) = ({self.n_qlayers}, {self.n_vrotations})\")\n",
    "\n",
    "        self.clayer_in = torch.nn.Linear(self.concat_size, self.n_inp_size)\n",
    "        self.VQC = {\n",
    "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
    "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
    "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
    "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes)\n",
    "        }\n",
    "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "        '''\n",
    "        x.shape is (batch_size, seq_length, feature_size)\n",
    "        recurrent_activation -> sigmoid\n",
    "        activation -> tanh\n",
    "        '''\n",
    "        if self.batch_first is True:\n",
    "            batch_size, seq_length, features_size = x.size()\n",
    "        else:\n",
    "            seq_length, batch_size, features_size = x.size()\n",
    "\n",
    "        hidden_seq = []\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
    "        else:\n",
    "            # for now we ignore the fact that in PyTorch you can stack multiple RNNs\n",
    "            # so we take only the first elements of the init_states tuple init_states[0][0], init_states[1][0]\n",
    "            h_t, c_t = init_states\n",
    "            h_t = h_t[0]\n",
    "            c_t = c_t[0]\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            # get features from the t-th element in seq, for all entries in the batch\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # Concatenate input and hidden state\n",
    "            v_t = torch.cat((h_t, x_t), dim=1)\n",
    "\n",
    "            # match qubit dimension\n",
    "            y_t = self.clayer_in(v_t)\n",
    "\n",
    "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](y_t)))  # forget block\n",
    "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](y_t)))  # input block\n",
    "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](y_t)))  # update block\n",
    "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](y_t))) # output block\n",
    "\n",
    "            c_t = (f_t * c_t) + (i_t * g_t)\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "\n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f27880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QShallowRegressionLSTM(nn.Module):\n",
    "    def __init__(self, num_sensors, hidden_units, n_qubits=6, n_qlayers=12): # n_qlayers decide the weights\n",
    "        super().__init__()\n",
    "        self.num_sensors = num_sensors  # this is the number of features\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 2 # Number of QLSTM layer\n",
    "\n",
    "        self.lstm = QLSTM(\n",
    "            input_size=num_sensors,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            n_qubits = n_qubits,\n",
    "            n_qlayers= n_qlayers\n",
    "        )\n",
    "\n",
    "        self.linear_1 = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "        self.linear_2 = nn.Linear(in_features=60, out_features=6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        \n",
    "        out_l1, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        out_l2 = self.linear_1(out_l1)\n",
    "        out = self.linear_2(torch.squeeze(out_l2))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ccb5d61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight_shapes = (n_qlayers, n_vrotations) = (12, 3)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.05\n",
    "num_hidden_units = 16\n",
    " \n",
    "Qmodel = QShallowRegressionLSTM(num_sensors=len(features), hidden_units=num_hidden_units, n_qubits=6) # Number of Qubits\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adagrad(Qmodel.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02505957",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "---------\n",
      "Train loss: 0.24071206104542528\n",
      "Execution time 11385.806482076645\n",
      "Epoch 1\n",
      "---------\n"
     ]
    }
   ],
   "source": [
    "quantum_loss_train = []\n",
    "quantum_loss_test = []\n",
    "num_epoch=5\n",
    "for ix_epoch in range(num_epoch):\n",
    "    print(f\"Epoch {ix_epoch}\\n---------\")\n",
    "    start = time.time()\n",
    "    train_loss = train_model(train_loader, Qmodel, loss_function, optimizer=optimizer)\n",
    "    end = time.time()\n",
    "    print(\"Execution time\", end - start)\n",
    "    quantum_loss_train.append(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa16f316-2b04-439a-b788-17a7d9bc45b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.24071206104542528,\n",
       " 0.0738448812670651,\n",
       " 0.056562427023337,\n",
       " 0.04531906358010712,\n",
       " 0.042639000350165934]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantum_loss_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148ffe2",
   "metadata": {},
   "source": [
    "# Predict for 8th to 15 march"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bd4e0ce-fb42-4985-9656-000a36b048cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "last_60 = pd.read_csv('last_60_days_input.csv')\n",
    "last_60 = last_60.drop([ 'Unnamed: 0'], axis=1)\n",
    "# Remove few redundant data\n",
    "last_60=last_60.drop(columns=['Date'])\n",
    "last_60=last_60.drop(columns=['CUMLOGRET_1'])\n",
    "last_60=last_60.drop(columns=['Gold in USD volume'])\n",
    "last_60=last_60.drop(columns=['Open'])\n",
    "last_60=last_60.drop(columns=['High'])\n",
    "last_60=last_60.drop(columns=['Low'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "687d0a7c-3ddf-4e04-bdd8-c998af31d2a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in features:\n",
    "    last_60[i]=(last_60[i]-mean[i])/stdev[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cf73ba71-0822-441b-a63c-967b18e9a558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_last_60=np.array(last_60[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8a5a48d-4a05-466a-9f5d-8a96030c2da9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21066/1693403120.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  y_8_to_15 = Qmodel(torch.FloatTensor([X_last_60]))\n"
     ]
    }
   ],
   "source": [
    "y_8_to_15 = Qmodel(torch.FloatTensor([X_last_60]))\n",
    "y_8_to_15 = y_8_to_15.detach().numpy()* target_stdev + target_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b67622db-17d3-4bea-8db3-d2365458dcc9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7662.464 , 7662.884 , 7645.0625, 7640.8667, 7660.0586, 7624.8257],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_8_to_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2014698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_params_Q = sum(p.numel() for p in Qmodel.parameters() if p.requires_grad)\n",
    "# print(\"No. of parameters for QLSTM: \", total_params_Q)\n",
    "# Number of parameter is 1323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca27a5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
